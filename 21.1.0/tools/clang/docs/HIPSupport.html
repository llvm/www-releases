<!DOCTYPE html>

<html lang="en" data-content_root="./">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>HIP Support &#8212; Clang 21.1.0 documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=649a27d8" />
    <link rel="stylesheet" type="text/css" href="_static/haiku.css?v=e491ac2d" />
    <link rel="stylesheet" type="text/css" href="_static/graphviz.css?v=eafc0fe6" />
    <script src="_static/documentation_options.js?v=c54a4614"></script>
    <script src="_static/doctools.js?v=888ff710"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="HLSL Design and Implementation" href="HLSL/HLSLDocs.html" />
    <link rel="prev" title="SYCL Compiler and Runtime architecture design" href="SYCLSupport.html" /> 
  </head><body>
      <div class="header" role="banner"><h1 class="heading"><a href="index.html">
          <span>Clang 21.1.0 documentation</span></a></h1>
        <h2 class="heading"><span>HIP Support</span></h2>
      </div>
      <div class="topnav" role="navigation" aria-label="top navigation">
      
        <p>
        «&#160;&#160;<a href="SYCLSupport.html">SYCL Compiler and Runtime architecture design</a>
        &#160;&#160;::&#160;&#160;
        <a class="uplink" href="index.html">Contents</a>
        &#160;&#160;::&#160;&#160;
        <a href="HLSL/HLSLDocs.html">HLSL Design and Implementation</a>&#160;&#160;»
        </p>

      </div>
      <div class="content" role="main">
        
        
  <style type="text/css">
  .none { background-color: #FFCCCC }
  .part { background-color: #FFFF99 }
  .good { background-color: #CCFF99 }
</style><nav class="contents local" id="contents" role="doc-toc">
<ul class="simple">
<li><p><a class="reference internal" href="#hip-support" id="id12">HIP Support</a></p>
<ul>
<li><p><a class="reference internal" href="#amd-gpu-support" id="id13">AMD GPU Support</a></p></li>
<li><p><a class="reference internal" href="#intel-gpu-support" id="id14">Intel GPU Support</a></p></li>
<li><p><a class="reference internal" href="#example-usage" id="id15">Example Usage</a></p></li>
<li><p><a class="reference internal" href="#path-setting-for-dependencies" id="id16">Path Setting for Dependencies</a></p>
<ul>
<li><p><a class="reference internal" href="#order-of-precedence-for-hip-path" id="id17">Order of Precedence for HIP Path</a></p></li>
<li><p><a class="reference internal" href="#order-of-precedence-for-device-library-path" id="id18">Order of Precedence for Device Library Path</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#predefined-macros" id="id19">Predefined Macros</a></p></li>
<li><p><a class="reference internal" href="#compilation-modes" id="id20">Compilation Modes</a></p>
<ul>
<li><p><a class="reference internal" href="#device-code-compilation" id="id21">Device Code Compilation</a></p></li>
<li><p><a class="reference internal" href="#host-code-compilation" id="id22">Host Code Compilation</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#syntax-difference-with-cuda" id="id23">Syntax Difference with CUDA</a></p>
<ul>
<li><p><a class="reference internal" href="#predefined-macros-for-differentiation" id="id24">Predefined Macros for Differentiation</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#function-pointers-support" id="id25">Function Pointers Support</a></p></li>
<li><p><a class="reference internal" href="#virtual-function-support" id="id26">Virtual Function Support</a></p>
<ul>
<li><p><a class="reference internal" href="#explanation" id="id27">Explanation</a></p></li>
<li><p><a class="reference internal" href="#id2" id="id28">Example Usage</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#host-and-device-attributes-of-default-destructors" id="id29">Host and Device Attributes of Default Destructors</a></p></li>
<li><p><a class="reference internal" href="#c-standard-parallelism-offload-support-compiler-and-runtime" id="id30">C++ Standard Parallelism Offload Support: Compiler And Runtime</a></p></li>
<li><p><a class="reference internal" href="#introduction" id="id31">Introduction</a></p></li>
<li><p><a class="reference internal" href="#algorithm-offload-what-why-where" id="id32">Algorithm Offload: What, Why, Where</a></p></li>
<li><p><a class="reference internal" href="#small-example" id="id33">Small Example</a></p></li>
<li><p><a class="reference internal" href="#implementation-general-view" id="id34">Implementation - General View</a></p></li>
<li><p><a class="reference internal" href="#implementation-driver" id="id35">Implementation - Driver</a></p></li>
<li><p><a class="reference internal" href="#implementation-front-end" id="id36">Implementation - Front-End</a></p></li>
<li><p><a class="reference internal" href="#implementation-middle-end" id="id37">Implementation - Middle-End</a></p></li>
<li><p><a class="reference internal" href="#implementation-forwarding-header" id="id38">Implementation - Forwarding Header</a></p></li>
<li><p><a class="reference internal" href="#id5" id="id39">Predefined Macros</a></p></li>
<li><p><a class="reference internal" href="#restrictions" id="id40">Restrictions</a></p></li>
<li><p><a class="reference internal" href="#current-support" id="id41">Current Support</a></p>
<ul>
<li><p><a class="reference internal" href="#hip-specific-elements" id="id42">HIP Specific Elements</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#open-questions-future-developments" id="id43">Open Questions / Future Developments</a></p></li>
<li><p><a class="reference internal" href="#spir-v-support-on-hipamd-toolchain" id="id44">SPIR-V Support on HIPAMD ToolChain</a></p>
<ul>
<li><p><a class="reference internal" href="#compilation-process" id="id45">Compilation Process</a></p>
<ul>
<li><p><a class="reference internal" href="#using-offload-arch-amdgcnspirv" id="id46">Using <code class="docutils literal notranslate"><span class="pre">--offload-arch=amdgcnspirv</span></code></a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#architecture-specific-macros" id="id47">Architecture Specific Macros</a></p></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
<section id="hip-support">
<h1><a class="toc-backref" href="#id12" role="doc-backlink">HIP Support</a><a class="headerlink" href="#hip-support" title="Link to this heading">¶</a></h1>
<p>HIP (Heterogeneous-Compute Interface for Portability) <a class="reference external" href="https://github.com/ROCm/HIP">https://github.com/ROCm/HIP</a> is
a C++ Runtime API and Kernel Language. It enables developers to create portable applications for
offloading computation to different hardware platforms from a single source code.</p>
<section id="amd-gpu-support">
<h2><a class="toc-backref" href="#id13" role="doc-backlink">AMD GPU Support</a><a class="headerlink" href="#amd-gpu-support" title="Link to this heading">¶</a></h2>
<p>Clang provides HIP support on AMD GPUs via the ROCm platform <a class="reference external" href="https://rocm.docs.amd.com/en/latest/#">https://rocm.docs.amd.com/en/latest/#</a>.
The ROCm runtime forms the base for HIP host APIs, while HIP device APIs are realized through HIP header
files and the ROCm device library. The Clang driver uses the HIPAMD toolchain to compile HIP device code
to AMDGPU ISA via the AMDGPU backend, or SPIR-V via the workflow outlined below.
The compiled code is then bundled and embedded in the host executables.</p>
</section>
<section id="intel-gpu-support">
<h2><a class="toc-backref" href="#id14" role="doc-backlink">Intel GPU Support</a><a class="headerlink" href="#intel-gpu-support" title="Link to this heading">¶</a></h2>
<p>Clang provides partial HIP support on Intel GPUs using the CHIP-Star project <a class="reference external" href="https://github.com/CHIP-SPV/chipStar">https://github.com/CHIP-SPV/chipStar</a>.
CHIP-Star implements the HIP runtime over oneAPI Level Zero or OpenCL runtime. The Clang driver uses the HIPSPV
toolchain to compile HIP device code into LLVM IR, which is subsequently translated to SPIR-V via the SPIR-V
backend or the out-of-tree LLVM-SPIRV translator. The SPIR-V is then bundled and embedded into the host executables.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>While Clang does not directly provide HIP support for NVIDIA GPUs and CPUs, these platforms are supported via other means:</p>
<ul class="simple">
<li><p>NVIDIA GPUs: HIP support is offered through the HIP project <a class="reference external" href="https://github.com/ROCm/HIP">https://github.com/ROCm/HIP</a>, which provides a header-only library for translating HIP runtime APIs into CUDA runtime APIs. The code is subsequently compiled using NVIDIA’s <cite>nvcc</cite>.</p></li>
<li><p>CPUs: HIP support is available through the HIP-CPU runtime library <a class="reference external" href="https://github.com/ROCm/HIP-CPU">https://github.com/ROCm/HIP-CPU</a>. This header-only library enables CPUs to execute unmodified HIP code.</p></li>
</ul>
</div>
</section>
<section id="example-usage">
<h2><a class="toc-backref" href="#id15" role="doc-backlink">Example Usage</a><a class="headerlink" href="#example-usage" title="Link to this heading">¶</a></h2>
<p>To compile a HIP program, use the following command:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>clang++<span class="w"> </span>-c<span class="w"> </span>--offload-arch<span class="o">=</span>gfx906<span class="w"> </span>-xhip<span class="w"> </span>sample.cpp<span class="w"> </span>-o<span class="w"> </span>sample.o
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">-xhip</span></code> option indicates that the source is a HIP program. If the file has a <code class="docutils literal notranslate"><span class="pre">.hip</span></code> extension,
Clang will automatically recognize it as a HIP program:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>clang++<span class="w"> </span>-c<span class="w"> </span>--offload-arch<span class="o">=</span>gfx906<span class="w"> </span>sample.hip<span class="w"> </span>-o<span class="w"> </span>sample.o
</pre></div>
</div>
<p>To link a HIP program, use this command:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>clang++<span class="w"> </span>--hip-link<span class="w"> </span>--offload-arch<span class="o">=</span>gfx906<span class="w"> </span>sample.o<span class="w"> </span>-o<span class="w"> </span>sample
</pre></div>
</div>
<p>In the above command, the <code class="docutils literal notranslate"><span class="pre">--hip-link</span></code> flag instructs Clang to link the HIP runtime library. However,
the use of this flag is unnecessary if a HIP input file is already present in your program.</p>
<p>For convenience, Clang also supports compiling and linking in a single step:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>clang++<span class="w"> </span>--offload-arch<span class="o">=</span>gfx906<span class="w"> </span>-xhip<span class="w"> </span>sample.cpp<span class="w"> </span>-o<span class="w"> </span>sample
</pre></div>
</div>
<p>In the above commands, <code class="docutils literal notranslate"><span class="pre">gfx906</span></code> is the GPU architecture that the code is being compiled for. The supported GPU
architectures can be found in the <a class="reference external" href="https://llvm.org/docs/AMDGPUUsage.html#processors">AMDGPU Processor Table</a>.
Alternatively, you can use the <code class="docutils literal notranslate"><span class="pre">amdgpu-arch</span></code> tool that comes with Clang to list the GPU architecture on your system:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>amdgpu-arch
</pre></div>
</div>
<p>You can use <code class="docutils literal notranslate"><span class="pre">--offload-arch=native</span></code> to automatically detect the GPU architectures on your system:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>clang++<span class="w"> </span>--offload-arch<span class="o">=</span>native<span class="w"> </span>-xhip<span class="w"> </span>sample.cpp<span class="w"> </span>-o<span class="w"> </span>sample
</pre></div>
</div>
</section>
<section id="path-setting-for-dependencies">
<h2><a class="toc-backref" href="#id16" role="doc-backlink">Path Setting for Dependencies</a><a class="headerlink" href="#path-setting-for-dependencies" title="Link to this heading">¶</a></h2>
<p>Compiling a HIP program depends on the HIP runtime and device library. The paths to the HIP runtime and device libraries
can be specified either using compiler options or environment variables. The paths can also be set through the ROCm path
if they follow the ROCm installation directory structure.</p>
<section id="order-of-precedence-for-hip-path">
<h3><a class="toc-backref" href="#id17" role="doc-backlink">Order of Precedence for HIP Path</a><a class="headerlink" href="#order-of-precedence-for-hip-path" title="Link to this heading">¶</a></h3>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">--hip-path</span></code> compiler option</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">HIP_PATH</span></code> environment variable <em>(use with caution)</em></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--rocm-path</span></code> compiler option</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ROCM_PATH</span></code> environment variable <em>(use with caution)</em></p></li>
<li><p>Default automatic detection (relative to Clang or at the default ROCm installation location)</p></li>
</ol>
</section>
<section id="order-of-precedence-for-device-library-path">
<h3><a class="toc-backref" href="#id18" role="doc-backlink">Order of Precedence for Device Library Path</a><a class="headerlink" href="#order-of-precedence-for-device-library-path" title="Link to this heading">¶</a></h3>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">--hip-device-lib-path</span></code> compiler option</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">HIP_DEVICE_LIB_PATH</span></code> environment variable <em>(use with caution)</em></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--rocm-path</span></code> compiler option</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ROCM_PATH</span></code> environment variable <em>(use with caution)</em></p></li>
<li><p>Default automatic detection (relative to Clang or at the default ROCm installation location)</p></li>
</ol>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Compiler Option</p></th>
<th class="head"><p>Environment Variable</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Default Value</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--rocm-path=&lt;path&gt;</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">ROCM_PATH</span></code></p></td>
<td><p>Specifies the ROCm installation path.</p></td>
<td><p>Automatic detection</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--hip-path=&lt;path&gt;</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">HIP_PATH</span></code></p></td>
<td><p>Specifies the HIP runtime installation path.</p></td>
<td><p>Determined by ROCm directory structure</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--hip-device-lib-path=&lt;path&gt;</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">HIP_DEVICE_LIB_PATH</span></code></p></td>
<td><p>Specifies the HIP device library installation path.</p></td>
<td><p>Determined by ROCm directory structure</p></td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>We recommend using the compiler options as the primary method for specifying these paths. While the environment variables <code class="docutils literal notranslate"><span class="pre">ROCM_PATH</span></code>, <code class="docutils literal notranslate"><span class="pre">HIP_PATH</span></code>, and <code class="docutils literal notranslate"><span class="pre">HIP_DEVICE_LIB_PATH</span></code> are supported, their use can lead to implicit dependencies that might cause issues in the long run. Use them with caution.</p>
</div>
</section>
</section>
<section id="predefined-macros">
<h2><a class="toc-backref" href="#id19" role="doc-backlink">Predefined Macros</a><a class="headerlink" href="#predefined-macros" title="Link to this heading">¶</a></h2>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Macro</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">__CLANG_RDC__</span></code></p></td>
<td><p>Defined when Clang is compiling code in Relocatable Device Code (RDC) mode. RDC, enabled with the <code class="docutils literal notranslate"><span class="pre">-fgpu-rdc</span></code> compiler option, is necessary for linking device codes across translation units.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">__HIP__</span></code></p></td>
<td><p>Defined when compiling with HIP language support, indicating that the code targets the HIP environment.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">__HIPCC__</span></code></p></td>
<td><p>Alias to <code class="docutils literal notranslate"><span class="pre">__HIP__</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">__HIP_DEVICE_COMPILE__</span></code></p></td>
<td><p>Defined during device code compilation in Clang’s separate compilation process for the host and each offloading GPU architecture.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">__HIP_MEMORY_SCOPE_SINGLETHREAD</span></code></p></td>
<td><p>Represents single-thread memory scope in HIP (value is 1).</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">__HIP_MEMORY_SCOPE_WAVEFRONT</span></code></p></td>
<td><p>Represents wavefront memory scope in HIP (value is 2).</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">__HIP_MEMORY_SCOPE_WORKGROUP</span></code></p></td>
<td><p>Represents workgroup memory scope in HIP (value is 3).</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">__HIP_MEMORY_SCOPE_AGENT</span></code></p></td>
<td><p>Represents agent memory scope in HIP (value is 4).</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">__HIP_MEMORY_SCOPE_SYSTEM</span></code></p></td>
<td><p>Represents system-wide memory scope in HIP (value is 5).</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">__HIP_NO_IMAGE_SUPPORT__</span></code></p></td>
<td><p>Defined with a value of 1 when the target device lacks support for HIP image functions.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">__HIP_NO_IMAGE_SUPPORT</span></code></p></td>
<td><p>Alias to <code class="docutils literal notranslate"><span class="pre">__HIP_NO_IMAGE_SUPPORT__</span></code>. Deprecated.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">__HIP_API_PER_THREAD_DEFAULT_STREAM__</span></code></p></td>
<td><p>Defined when the GPU default stream is set to per-thread mode.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">HIP_API_PER_THREAD_DEFAULT_STREAM</span></code></p></td>
<td><p>Alias to <code class="docutils literal notranslate"><span class="pre">__HIP_API_PER_THREAD_DEFAULT_STREAM__</span></code>. Deprecated.</p></td>
</tr>
</tbody>
</table>
<p>Note that some architecture specific AMDGPU macros will have default values when
used from the HIP host compilation. Other <a class="reference internal" href="AMDGPUSupport.html"><span class="doc">AMDGPU macros</span></a>
like <code class="docutils literal notranslate"><span class="pre">__AMDGCN_WAVEFRONT_SIZE__</span></code> (deprecated) will default to 64 for example.</p>
</section>
<section id="compilation-modes">
<h2><a class="toc-backref" href="#id20" role="doc-backlink">Compilation Modes</a><a class="headerlink" href="#compilation-modes" title="Link to this heading">¶</a></h2>
<p>Each HIP source file contains intertwined device and host code. Depending on the chosen compilation mode by the compiler options <code class="docutils literal notranslate"><span class="pre">-fno-gpu-rdc</span></code> and <code class="docutils literal notranslate"><span class="pre">-fgpu-rdc</span></code>, these portions of code are compiled differently.</p>
<section id="device-code-compilation">
<h3><a class="toc-backref" href="#id21" role="doc-backlink">Device Code Compilation</a><a class="headerlink" href="#device-code-compilation" title="Link to this heading">¶</a></h3>
<p><strong>``-fno-gpu-rdc`` Mode (default)</strong>:</p>
<ul class="simple">
<li><p>Compiles to a self-contained, fully linked offloading device binary for each offloading device architecture.</p></li>
<li><p>Device code within a Translation Unit (TU) cannot call functions located in another TU.</p></li>
</ul>
<p><strong>``-fgpu-rdc`` Mode</strong>:</p>
<ul class="simple">
<li><p>Compiles to a bitcode for each GPU architecture.</p></li>
<li><p>For each offloading device architecture, the bitcode from different TUs are linked together to create a single offloading device binary.</p></li>
<li><p>Device code in one TU can call functions located in another TU.</p></li>
</ul>
</section>
<section id="host-code-compilation">
<h3><a class="toc-backref" href="#id22" role="doc-backlink">Host Code Compilation</a><a class="headerlink" href="#host-code-compilation" title="Link to this heading">¶</a></h3>
<p><strong>Both Modes</strong>:</p>
<ul class="simple">
<li><p>Compiles to a relocatable object for each TU.</p></li>
<li><p>These relocatable objects are then linked together.</p></li>
<li><p>Host code within a TU can call host functions and launch kernels from another TU.</p></li>
</ul>
</section>
</section>
<section id="syntax-difference-with-cuda">
<h2><a class="toc-backref" href="#id23" role="doc-backlink">Syntax Difference with CUDA</a><a class="headerlink" href="#syntax-difference-with-cuda" title="Link to this heading">¶</a></h2>
<p>Clang’s front end, used for both CUDA and HIP programming models, shares the same parsing and semantic analysis mechanisms. This includes the resolution of overloads concerning device and host functions. While there exists a comprehensive documentation on the syntax differences between Clang and NVCC for CUDA at <a class="reference external" href="https://llvm.org/docs/CompileCudaWithLLVM.html#dialect-differences-between-clang-and-nvcc">Dialect Differences Between Clang and NVCC</a>, it is important to note that these differences also apply to HIP code compilation.</p>
<section id="predefined-macros-for-differentiation">
<h3><a class="toc-backref" href="#id24" role="doc-backlink">Predefined Macros for Differentiation</a><a class="headerlink" href="#predefined-macros-for-differentiation" title="Link to this heading">¶</a></h3>
<p>To facilitate differentiation between HIP and CUDA code, as well as between device and host compilations within HIP, Clang defines specific macros:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">__HIP__</span></code> : This macro is defined only when compiling HIP code. It can be used to conditionally compile code specific to HIP, enabling developers to write portable code that can be compiled for both CUDA and HIP.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">__HIP_DEVICE_COMPILE__</span></code> : Defined exclusively during HIP device compilation, this macro allows for conditional compilation of device-specific code. It provides a mechanism to segregate device and host code, ensuring that each can be optimized for their respective execution environments.</p></li>
</ul>
</section>
</section>
<section id="function-pointers-support">
<h2><a class="toc-backref" href="#id25" role="doc-backlink">Function Pointers Support</a><a class="headerlink" href="#function-pointers-support" title="Link to this heading">¶</a></h2>
<p>Function pointers’ support varies with the usage mode in Clang with HIP. The following table provides an overview of the support status across different use-cases and modes.</p>
<table class="docutils align-default" id="id11">
<caption><span class="caption-text">Function Pointers Support Overview</span><a class="headerlink" href="#id11" title="Link to this table">¶</a></caption>
<colgroup>
<col style="width: 33.3%" />
<col style="width: 33.3%" />
<col style="width: 33.3%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Use Case</p></th>
<th class="head"><p><code class="docutils literal notranslate"><span class="pre">-fno-gpu-rdc</span></code> Mode (default)</p></th>
<th class="head"><p><code class="docutils literal notranslate"><span class="pre">-fgpu-rdc</span></code> Mode</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Defined and used in the same TU</p></td>
<td><p>Supported</p></td>
<td><p>Supported</p></td>
</tr>
<tr class="row-odd"><td><p>Defined in one TU and used in another TU</p></td>
<td><p>Not Supported</p></td>
<td><p>Supported</p></td>
</tr>
</tbody>
</table>
<p>In the <code class="docutils literal notranslate"><span class="pre">-fno-gpu-rdc</span></code> mode, the compiler calculates the resource usage of kernels based only on functions present within the same TU. This mode does not support the use of function pointers defined in a different TU due to the possibility of incorrect resource usage calculations, leading to undefined behavior.</p>
<p>On the other hand, the <code class="docutils literal notranslate"><span class="pre">-fgpu-rdc</span></code> mode allows the definition and use of function pointers across different TUs, as resource usage calculations can accommodate functions from disparate TUs.</p>
</section>
<section id="virtual-function-support">
<h2><a class="toc-backref" href="#id26" role="doc-backlink">Virtual Function Support</a><a class="headerlink" href="#virtual-function-support" title="Link to this heading">¶</a></h2>
<p>In Clang with HIP, support for calling virtual functions of an object in device or host code is contingent on where the object is constructed.</p>
<ul class="simple">
<li><p><strong>Constructed in Device Code</strong>: Virtual functions of an object can be called in device code on a specific offloading device if the object is constructed in device code on an offloading device with the same architecture.</p></li>
<li><p><strong>Constructed in Host Code</strong>: Virtual functions of an object can be called in host code if the object is constructed in host code.</p></li>
</ul>
<p>In other scenarios, calling virtual functions is not allowed.</p>
<section id="explanation">
<h3><a class="toc-backref" href="#id27" role="doc-backlink">Explanation</a><a class="headerlink" href="#explanation" title="Link to this heading">¶</a></h3>
<p>An object constructed on the device side contains a pointer to the virtual function table on the device side, which is not accessible in host code, and vice versa. Thus, trying to invoke virtual functions from a context different from where the object was constructed will be disallowed because the appropriate virtual table cannot be accessed. The virtual function tables for offloading devices with different architectures are different, therefore trying to invoke virtual functions from an offloading device with a different architecture than where the object is constructed is also disallowed.</p>
</section>
<section id="id2">
<h3><a class="toc-backref" href="#id28" role="doc-backlink">Example Usage</a><a class="headerlink" href="#id2" title="Link to this heading">¶</a></h3>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Base</span><span class="w"> </span><span class="p">{</span>
<span class="k">public</span><span class="o">:</span>
<span class="w">   </span><span class="n">__device__</span><span class="w"> </span><span class="k">virtual</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">virtualFunction</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="c1">// Base virtual function implementation</span>
<span class="w">   </span><span class="p">}</span>
<span class="p">};</span>

<span class="k">class</span><span class="w"> </span><span class="nc">Derived</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="k">public</span><span class="w"> </span><span class="n">Base</span><span class="w"> </span><span class="p">{</span>
<span class="k">public</span><span class="o">:</span>
<span class="w">   </span><span class="n">__device__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">virtualFunction</span><span class="p">()</span><span class="w"> </span><span class="k">override</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="c1">// Derived virtual function implementation</span>
<span class="w">   </span><span class="p">}</span>
<span class="p">};</span>

<span class="n">__global__</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">kernel</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">   </span><span class="n">Derived</span><span class="w"> </span><span class="n">obj</span><span class="p">;</span>
<span class="w">   </span><span class="n">Base</span><span class="o">*</span><span class="w"> </span><span class="n">basePtr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">&amp;</span><span class="n">obj</span><span class="p">;</span>
<span class="w">   </span><span class="n">basePtr</span><span class="o">-&gt;</span><span class="n">virtualFunction</span><span class="p">();</span><span class="w"> </span><span class="c1">// Allowed since obj is constructed in device code</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
</section>
<section id="host-and-device-attributes-of-default-destructors">
<h2><a class="toc-backref" href="#id29" role="doc-backlink">Host and Device Attributes of Default Destructors</a><a class="headerlink" href="#host-and-device-attributes-of-default-destructors" title="Link to this heading">¶</a></h2>
<p>If a default destructor does not have explicit host or device attributes,
clang infers these attributes based on the destructors of its data members
and base classes. If any conflicts are detected among these destructors,
clang diagnoses the issue. Otherwise, clang adds an implicit host or device
attribute according to whether the data members’s and base classes’s
destructors can execute on the host or device side.</p>
<p>For explicit template classes with virtual destructors, which must be emitted,
the inference adopts a conservative approach. In this case, implicit host or
device attributes from member and base class destructors are ignored. This
precaution is necessary because, although a constexpr destructor carries
implicit host or device attributes, a constexpr function may call a
non-constexpr function, which is by default a host function.</p>
<p>Users can override the inferred host and device attributes of default
destructors by adding explicit host and device attributes to them.</p>
</section>
<section id="c-standard-parallelism-offload-support-compiler-and-runtime">
<h2><a class="toc-backref" href="#id30" role="doc-backlink">C++ Standard Parallelism Offload Support: Compiler And Runtime</a><a class="headerlink" href="#c-standard-parallelism-offload-support-compiler-and-runtime" title="Link to this heading">¶</a></h2>
</section>
<section id="introduction">
<h2><a class="toc-backref" href="#id31" role="doc-backlink">Introduction</a><a class="headerlink" href="#introduction" title="Link to this heading">¶</a></h2>
<p>This section describes the implementation of support for offloading the
execution of standard C++ algorithms to accelerators that can be targeted via
HIP. Furthermore, it enumerates restrictions on user defined code, as well as
the interactions with runtimes.</p>
</section>
<section id="algorithm-offload-what-why-where">
<h2><a class="toc-backref" href="#id32" role="doc-backlink">Algorithm Offload: What, Why, Where</a><a class="headerlink" href="#algorithm-offload-what-why-where" title="Link to this heading">¶</a></h2>
<p>C++17 introduced overloads
<a class="reference external" href="https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/p0024r2.html">for most algorithms in the standard library</a>
which allow the user to specify a desired
<a class="reference external" href="https://en.cppreference.com/w/cpp/algorithm#Execution_policies">execution policy</a>.
The <a class="reference external" href="https://en.cppreference.com/w/cpp/algorithm/execution_policy_tag_t">parallel_unsequenced_policy</a>
maps relatively well to the execution model of AMD GPUs. This, coupled with the
the availability and maturity of GPU accelerated algorithm libraries that
implement most / all corresponding algorithms in the standard library
(e.g. <a class="reference external" href="https://github.com/ROCm/rocm-libraries/tree/develop/projects/rocthrust">rocThrust</a>), makes
it feasible to provide seamless accelerator offload for supported algorithms,
when an accelerated version exists. Thus, it becomes possible to easily access
the computational resources of an AMD accelerator, via a well specified,
familiar, algorithmic interface, without having to delve into low-level hardware
specific details. Putting it all together:</p>
<ul class="simple">
<li><p><strong>What</strong>: standard library algorithms, when invoked with the
<code class="docutils literal notranslate"><span class="pre">parallel_unsequenced_policy</span></code></p></li>
<li><p><strong>Why</strong>: democratise AMDGPU accelerator programming, without loss of user
familiarity</p></li>
<li><p><strong>Where</strong>: only AMDGPU accelerators targeted by Clang/LLVM via HIP</p></li>
</ul>
</section>
<section id="small-example">
<h2><a class="toc-backref" href="#id33" role="doc-backlink">Small Example</a><a class="headerlink" href="#small-example" title="Link to this heading">¶</a></h2>
<p>Given the following C++ code:</p>
<div class="highlight-C++ notranslate"><div class="highlight"><pre><span></span><span class="kt">bool</span><span class="w"> </span><span class="nf">has_the_answer</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;&amp;</span><span class="w"> </span><span class="n">v</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">find</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">execution</span><span class="o">::</span><span class="n">par_unseq</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">cbegin</span><span class="p">(</span><span class="n">v</span><span class="p">),</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">cend</span><span class="p">(</span><span class="n">v</span><span class="p">),</span><span class="w"> </span><span class="mi">42</span><span class="p">)</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">cend</span><span class="p">(</span><span class="n">v</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
<p>if Clang is invoked with the <code class="docutils literal notranslate"><span class="pre">--hipstdpar</span> <span class="pre">--offload-arch=foo</span></code> flags, the call
to <code class="docutils literal notranslate"><span class="pre">find</span></code> will be offloaded to an accelerator that is part of the <code class="docutils literal notranslate"><span class="pre">foo</span></code>
target family. If either <code class="docutils literal notranslate"><span class="pre">foo</span></code> or its runtime environment do not support
transparent on-demand paging (such as e.g. that provided in Linux via
<a class="reference external" href="https://docs.kernel.org/mm/hmm.html">HMM</a>), it is necessary to also include
the <code class="docutils literal notranslate"><span class="pre">--hipstdpar-interpose-alloc</span></code> flag. If the accelerator specific algorithm
library <code class="docutils literal notranslate"><span class="pre">foo</span></code> uses doesn’t have an implementation of a particular algorithm,
execution seamlessly falls back to the host CPU. It is legal to specify multiple
<code class="docutils literal notranslate"><span class="pre">--offload-arch</span></code>s. All the flags we introduce, as well as a thorough view of
various restrictions an their implementations, will be provided below.</p>
</section>
<section id="implementation-general-view">
<h2><a class="toc-backref" href="#id34" role="doc-backlink">Implementation - General View</a><a class="headerlink" href="#implementation-general-view" title="Link to this heading">¶</a></h2>
<p>We built support for Algorithm Offload support atop the pre-existing HIP
infrastructure. More specifically, when one requests offload via <code class="docutils literal notranslate"><span class="pre">--hipstdpar</span></code>,
compilation is switched to HIP compilation, as if <code class="docutils literal notranslate"><span class="pre">-x</span> <span class="pre">hip</span></code> was specified.
Similarly, linking is also switched to HIP linking, as if <code class="docutils literal notranslate"><span class="pre">--hip-link</span></code> was
specified. Note that these are implicit, and one should not assume that any
interop with HIP specific language constructs is available e.g. <code class="docutils literal notranslate"><span class="pre">__device__</span></code>
annotations are neither necessary nor guaranteed to work.</p>
<p>Since there are no language restriction mechanisms in place, it is necessary to
relax HIP language specific semantic checks performed by the FE; they would
identify otherwise valid, offloadable code, as invalid HIP code. Given that we
know that the user intended only for certain algorithms to be offloaded, and
encoded this by specifying the <code class="docutils literal notranslate"><span class="pre">parallel_unsequenced_policy</span></code>, we rely on a
pass over IR to clean up any and all code that was not “meant” for offload. If
requested, allocation interposition is also handled via a separate pass over IR.</p>
<p>To interface with the client HIP runtime, and to forward offloaded algorithm
invocations to the corresponding accelerator specific library implementation, an
implementation detail forwarding header is implicitly included by the driver,
when compiling with <code class="docutils literal notranslate"><span class="pre">--hipstdpar</span></code>. In what follows, we will delve into each
component that contributes to implementing Algorithm Offload support.</p>
</section>
<section id="implementation-driver">
<h2><a class="toc-backref" href="#id35" role="doc-backlink">Implementation - Driver</a><a class="headerlink" href="#implementation-driver" title="Link to this heading">¶</a></h2>
<p>We augment the <code class="docutils literal notranslate"><span class="pre">clang</span></code> driver with the following flags:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">--hipstdpar</span></code> enables algorithm offload, which depending on phase, has the
following effects:</p>
<ul>
<li><p>when compiling:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">-x</span> <span class="pre">hip</span></code> gets prepended to enable HIP support;</p></li>
<li><p>the <code class="docutils literal notranslate"><span class="pre">ROCmToolchain</span></code> component checks for the <code class="docutils literal notranslate"><span class="pre">hipstdpar_lib.hpp</span></code>
forwarding header,
<a class="reference external" href="https://rocm.docs.amd.com/projects/rocThrust/en/latest/">rocThrust</a> and
<a class="reference external" href="https://rocm.docs.amd.com/projects/rocPRIM/en/latest/">rocPrim</a> in
their canonical locations, which can be overriden via flags found below;
if all are found, the forwarding header gets implicitly included,
otherwise an error listing the missing component is generated;</p></li>
<li><p>the <code class="docutils literal notranslate"><span class="pre">LangOpts.HIPStdPar</span></code> member is set.</p></li>
</ul>
</li>
<li><p>when linking:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">--hip-link</span></code> and <code class="docutils literal notranslate"><span class="pre">-frtlib-add-rpath</span></code> gets appended to enable HIP
support.</p></li>
</ul>
</li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">--hipstdpar-interpose-alloc</span></code> enables the interposition of standard
allocation / deallocation functions with accelerator aware equivalents; the
<code class="docutils literal notranslate"><span class="pre">LangOpts.HIPStdParInterposeAlloc</span></code> member is set;</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--hipstdpar-path=</span></code> specifies a non-canonical path for the forwarding
header; it must point to the folder where the header is located and not to the
header itself;</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--hipstdpar-thrust-path=</span></code> specifies a non-canonical path for
<a class="reference external" href="https://rocm.docs.amd.com/projects/rocThrust/en/latest/">rocThrust</a>; it
must point to the folder where the library is installed / built under a
<code class="docutils literal notranslate"><span class="pre">/thrust</span></code> subfolder;</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--hipstdpar-prim-path=</span></code> specifies a non-canonical path for
<a class="reference external" href="https://rocm.docs.amd.com/projects/rocPRIM/en/latest/">rocPrim</a>; it must
point to the folder where the library is installed / built under a
<code class="docutils literal notranslate"><span class="pre">/rocprim</span></code> subfolder;</p></li>
</ul>
<p>The <a class="reference external" href="https://llvm.org/docs/AMDGPUUsage.html#amdgpu-processors">–offload-arch</a>
flag can be used to specify the accelerator for which offload code is to be
generated.</p>
</section>
<section id="implementation-front-end">
<h2><a class="toc-backref" href="#id36" role="doc-backlink">Implementation - Front-End</a><a class="headerlink" href="#implementation-front-end" title="Link to this heading">¶</a></h2>
<p>When <code class="docutils literal notranslate"><span class="pre">LangOpts.HIPStdPar</span></code> is set, we relax some of the HIP language specific
<code class="docutils literal notranslate"><span class="pre">Sema</span></code> checks to account for the fact that we want to consume pure unannotated
C++ code:</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">__device__</span></code> / <code class="docutils literal notranslate"><span class="pre">__host__</span> <span class="pre">__device__</span></code> functions (which would originate in
the accelerator specific algorithm library) are allowed to call implicitly
<code class="docutils literal notranslate"><span class="pre">__host__</span></code> functions;</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">__global__</span></code> functions (which would originate in the accelerator specific
algorithm library) are allowed to call implicitly <code class="docutils literal notranslate"><span class="pre">__host__</span></code> functions;</p></li>
<li><p>resolving <code class="docutils literal notranslate"><span class="pre">__builtin</span></code> availability is deferred, because it is possible that
a <code class="docutils literal notranslate"><span class="pre">__builtin</span></code> that is unavailable on the target accelerator is not
reachable from any offloaded algorithm, and thus will be safely removed in
the middle-end;</p></li>
<li><p>ASM parsing / checking is deferred, because it is possible that an ASM block
that e.g. uses some constraints that are incompatible with the target
accelerator is not reachable from any offloaded algorithm, and thus will be
safely removed in the middle-end.</p></li>
</ol>
<p><code class="docutils literal notranslate"><span class="pre">CodeGen</span></code> is similarly relaxed, with implicitly <code class="docutils literal notranslate"><span class="pre">__host__</span></code> functions being
emitted as well.</p>
</section>
<section id="implementation-middle-end">
<h2><a class="toc-backref" href="#id37" role="doc-backlink">Implementation - Middle-End</a><a class="headerlink" href="#implementation-middle-end" title="Link to this heading">¶</a></h2>
<p>We add two <code class="docutils literal notranslate"><span class="pre">opt</span></code> passes:</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">HipStdParAcceleratorCodeSelectionPass</span></code></p>
<ul class="simple">
<li><p>For all kernels in a <code class="docutils literal notranslate"><span class="pre">Module</span></code>, compute reachability, where a function
<code class="docutils literal notranslate"><span class="pre">F</span></code> is reachable from a kernel <code class="docutils literal notranslate"><span class="pre">K</span></code> if and only if there exists a direct
call-chain rooted in <code class="docutils literal notranslate"><span class="pre">F</span></code> that includes <code class="docutils literal notranslate"><span class="pre">K</span></code>;</p></li>
<li><p>Remove all functions that are not reachable from kernels;</p></li>
<li><p>This pass is only run when compiling for the accelerator.</p></li>
</ul>
</li>
</ol>
<p>The first pass assumes that the only code that the user intended to offload was
that which was directly or transitively invocable as part of an algorithm
execution. It also assumes that an accelerator aware algorithm implementation
would rely on accelerator specific special functions (kernels), and that these
effectively constitute the only roots for accelerator execution graphs. Both of
these assumptions are based on observing how widespread accelerators,
such as GPUs, work.</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">HipStdParAllocationInterpositionPass</span></code></p>
<ul class="simple">
<li><p>Iterate through all functions in a <code class="docutils literal notranslate"><span class="pre">Module</span></code>, and replace standard
allocation / deallocation functions with accelerator-aware equivalents,
based on a pre-established table; the list of functions that can be
interposed is available
<a class="reference external" href="https://github.com/ROCm/roc-stdpar#allocation--deallocation-interposition-status">here</a>;</p></li>
<li><p>This is only run when compiling for the host.</p></li>
</ul>
</li>
</ol>
<p>The second pass is optional.</p>
</section>
<section id="implementation-forwarding-header">
<h2><a class="toc-backref" href="#id38" role="doc-backlink">Implementation - Forwarding Header</a><a class="headerlink" href="#implementation-forwarding-header" title="Link to this heading">¶</a></h2>
<p>The forwarding header implements two pieces of functionality:</p>
<ol class="arabic simple">
<li><p>It forwards algorithms to a target accelerator, which is done by relying on
C++ language rules around overloading:</p>
<ul class="simple">
<li><p>overloads taking an explicit argument of type
<code class="docutils literal notranslate"><span class="pre">parallel_unsequenced_policy</span></code> are introduced into the <code class="docutils literal notranslate"><span class="pre">std</span></code> namespace;</p></li>
<li><p>these will get preferentially selected versus the master template;</p></li>
<li><p>the body forwards to the equivalent algorithm from the accelerator specific
library</p></li>
</ul>
</li>
<li><p>It provides allocation / deallocation functions that are equivalent to the
standard ones, but obtain memory by invoking
<a class="reference external" href="https://rocm.docs.amd.com/projects/HIP/en/latest/.doxygen/docBin/html/group___memory_m.html#gab8cfa0e292193fa37e0cc2e4911fa90a">hipMallocManaged</a>
and release it via <a class="reference external" href="https://rocm.docs.amd.com/projects/HIP/en/latest/.doxygen/docBin/html/group___memory.html#ga740d08da65cae1441ba32f8fedb863d1">hipFree</a>.</p></li>
</ol>
</section>
<section id="id5">
<h2><a class="toc-backref" href="#id39" role="doc-backlink">Predefined Macros</a><a class="headerlink" href="#id5" title="Link to this heading">¶</a></h2>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Macro</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">__HIPSTDPAR__</span></code></p></td>
<td><p>Defined when Clang is compiling code in algorithm offload mode, enabled
with the <code class="docutils literal notranslate"><span class="pre">--hipstdpar</span></code> compiler option.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">__HIPSTDPAR_INTERPOSE_ALLOC__</span></code> / <code class="docutils literal notranslate"><span class="pre">__HIPSTDPAR_INTERPOSE_ALLOC_V1__</span></code></p></td>
<td><p>Defined only when compiling in algorithm offload mode, when the user
enables interposition mode with the <code class="docutils literal notranslate"><span class="pre">--hipstdpar-interpose-alloc</span></code>
compiler option, indicating that all dynamic memory allocation /
deallocation functions should be replaced with accelerator aware
variants.</p></td>
</tr>
</tbody>
</table>
</section>
<section id="restrictions">
<h2><a class="toc-backref" href="#id40" role="doc-backlink">Restrictions</a><a class="headerlink" href="#restrictions" title="Link to this heading">¶</a></h2>
<p>We define two modes in which runtime execution can occur:</p>
<ol class="arabic simple">
<li><p><strong>HMM Mode</strong> - this assumes that the
<a class="reference external" href="https://docs.kernel.org/mm/hmm.html">HMM</a> subsystem of the Linux kernel
is used to provide transparent on-demand paging i.e. memory obtained from a
system / OS allocator such as via a call to <code class="docutils literal notranslate"><span class="pre">malloc</span></code> or <code class="docutils literal notranslate"><span class="pre">operator</span> <span class="pre">new</span></code> is
directly accessible to the accelerator and it follows the C++ memory model;</p></li>
<li><p><strong>Interposition Mode</strong> - this is a fallback mode for cases where transparent
on-demand paging is unavailable (e.g. in the Windows OS), which means that
memory must be allocated via an accelerator aware mechanism, and system
allocated memory is inaccessible for the accelerator.</p></li>
</ol>
<p>The following restrictions imposed on user code apply to both modes:</p>
<ol class="arabic">
<li><p>Pointers to function, and all associated features, such as e.g. dynamic
polymorphism, cannot be used (directly or transitively) by the user provided
callable passed to an algorithm invocation;</p></li>
<li><p>Global / namespace scope / <code class="docutils literal notranslate"><span class="pre">static</span></code> / <code class="docutils literal notranslate"><span class="pre">thread</span></code> storage duration variables
cannot be used (directly or transitively) in name by the user provided
callable;</p>
<ul>
<li><p>When executing in <strong>HMM Mode</strong> they can be used in address e.g.:</p>
<div class="highlight-C++ notranslate"><div class="highlight"><pre><span></span><span class="k">namespace</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">foo</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">42</span><span class="p">;</span><span class="w"> </span><span class="p">}</span>

<span class="kt">bool</span><span class="w"> </span><span class="n">never</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;&amp;</span><span class="w"> </span><span class="n">v</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">any_of</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">execution</span><span class="o">::</span><span class="n">par_unseq</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">cbegin</span><span class="p">(</span><span class="n">v</span><span class="p">),</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">cend</span><span class="p">(</span><span class="n">v</span><span class="p">),</span><span class="w"> </span><span class="p">[](</span><span class="k">auto</span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">foo</span><span class="p">;</span>
<span class="w">  </span><span class="p">});</span>
<span class="p">}</span>

<span class="kt">bool</span><span class="w"> </span><span class="n">only_in_hmm_mode</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;&amp;</span><span class="w"> </span><span class="n">v</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">any_of</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">execution</span><span class="o">::</span><span class="n">par_unseq</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">cbegin</span><span class="p">(</span><span class="n">v</span><span class="p">),</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">cend</span><span class="p">(</span><span class="n">v</span><span class="p">),</span>
<span class="w">                     </span><span class="p">[</span><span class="n">p</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">&amp;</span><span class="n">foo</span><span class="p">](</span><span class="k">auto</span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="o">*</span><span class="n">p</span><span class="p">;</span><span class="w"> </span><span class="p">});</span>
<span class="p">}</span>
</pre></div>
</div>
</li>
</ul>
</li>
<li><p>Only algorithms that are invoked with the <code class="docutils literal notranslate"><span class="pre">parallel_unsequenced_policy</span></code> are
candidates for offload;</p></li>
<li><p>Only algorithms that are invoked with iterator arguments that model
<a class="reference external" href="https://en.cppreference.com/w/cpp/iterator/random_access_iterator">random_access_iterator</a>
are candidates for offload;</p></li>
<li><p><a class="reference external" href="https://en.cppreference.com/w/cpp/language/exceptions">Exceptions</a> cannot
be used by the user provided callable;</p></li>
<li><p>Dynamic memory allocation (e.g. <code class="docutils literal notranslate"><span class="pre">operator</span> <span class="pre">new</span></code>) cannot be used by the user
provided callable;</p></li>
<li><p>Selective offload is not possible i.e. it is not possible to indicate that
only some algorithms invoked with the <code class="docutils literal notranslate"><span class="pre">parallel_unsequenced_policy</span></code> are to
be executed on the accelerator.</p></li>
</ol>
<p>In addition to the above, using <strong>Interposition Mode</strong> imposes the following
additional restrictions:</p>
<ol class="arabic">
<li><p>All code that is expected to interoperate has to be recompiled with the
<code class="docutils literal notranslate"><span class="pre">--hipstdpar-interpose-alloc</span></code> flag i.e. it is not safe to compose libraries
that have been independently compiled;</p></li>
<li><p>automatic storage duration (i.e. stack allocated) variables cannot be used
(directly or transitively) by the user provided callable e.g.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="kt">bool</span><span class="w"> </span><span class="nf">never</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;&amp;</span><span class="w"> </span><span class="n">v</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">n</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">any_of</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">execution</span><span class="o">::</span><span class="n">par_unseq</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">cbegin</span><span class="p">(</span><span class="n">v</span><span class="p">),</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">cend</span><span class="p">(</span><span class="n">v</span><span class="p">),</span>
<span class="w">                     </span><span class="p">[</span><span class="n">p</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">&amp;</span><span class="n">n</span><span class="p">](</span><span class="k">auto</span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="o">*</span><span class="n">p</span><span class="p">;</span><span class="w"> </span><span class="p">});</span>
<span class="p">}</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="current-support">
<h2><a class="toc-backref" href="#id41" role="doc-backlink">Current Support</a><a class="headerlink" href="#current-support" title="Link to this heading">¶</a></h2>
<p>At the moment, C++ Standard Parallelism Offload is only available for AMD GPUs,
when the <a class="reference external" href="https://rocm.docs.amd.com/en/latest/">ROCm</a> stack is used, on the
Linux operating system. Support is synthesised in the following table:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p><a class="reference external" href="https://llvm.org/docs/AMDGPUUsage.html#amdgpu-processors">Processor</a></p></th>
<th class="head"><p>HMM Mode</p></th>
<th class="head"><p>Interposition Mode</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>GCN GFX9 (Vega)</p></td>
<td><p>YES</p></td>
<td><p>YES</p></td>
</tr>
<tr class="row-odd"><td><p>GCN GFX10.1 (RDNA 1)</p></td>
<td><p><em>NO</em></p></td>
<td><p>YES</p></td>
</tr>
<tr class="row-even"><td><p>GCN GFX10.3 (RDNA 2)</p></td>
<td><p><em>NO</em></p></td>
<td><p>YES</p></td>
</tr>
<tr class="row-odd"><td><p>GCN GFX11 (RDNA 3)</p></td>
<td><p><em>NO</em></p></td>
<td><p>YES</p></td>
</tr>
<tr class="row-even"><td><p>GCN GFX12 (RDNA 4)</p></td>
<td><p><em>NO</em></p></td>
<td><p>YES</p></td>
</tr>
</tbody>
</table>
<p>The minimum Linux kernel version for running in HMM mode is 6.4.</p>
<p>The forwarding header can be obtained from
<a class="reference external" href="https://github.com/ROCm/roc-stdpar">its GitHub repository</a>.
It will be packaged with a future <a class="reference external" href="https://rocm.docs.amd.com/en/latest/">ROCm</a>
release. Because accelerated algorithms are provided via
<a class="reference external" href="https://rocm.docs.amd.com/projects/rocThrust/en/latest/">rocThrust</a>, a
transitive dependency on
<a class="reference external" href="https://rocm.docs.amd.com/projects/rocPRIM/en/latest/">rocPrim</a> exists. Both
can be obtained either by installing their associated components of the
<a class="reference external" href="https://rocm.docs.amd.com/en/latest/">ROCm</a> stack, or from their respective
repositories. The list algorithms that can be offloaded is available
<a class="reference external" href="https://github.com/ROCm/roc-stdpar#algorithm-support-status">here</a>.</p>
<section id="hip-specific-elements">
<h3><a class="toc-backref" href="#id42" role="doc-backlink">HIP Specific Elements</a><a class="headerlink" href="#hip-specific-elements" title="Link to this heading">¶</a></h3>
<ol class="arabic">
<li><p>There is no defined interop with the
<a class="reference external" href="https://rocm.docs.amd.com/projects/HIP/en/latest/reference/kernel_language.html">HIP kernel language</a>;
whilst things like using <cite>__device__</cite> annotations might accidentally “work”,
they are not guaranteed to, and thus cannot be relied upon by user code;</p>
<ul class="simple">
<li><p>A consequence of the above is that both bitcode linking and linking
relocatable object files will “work”, but it is not guaranteed to remain
working or actively tested at the moment; this restriction might be relaxed
in the future.</p></li>
</ul>
</li>
<li><p>Combining explicit HIP, CUDA or OpenMP Offload compilation with
<code class="docutils literal notranslate"><span class="pre">--hipstdpar</span></code> based offloading is not allowed or supported in any way.</p></li>
<li><p>There is no way to target different accelerators via a standard algorithm
invocation (<a class="reference external" href="https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2023/p2500r1.html">this might be addressed in future C++ standards</a>);
an unsafe (per the point above) way of achieving this is to spawn new threads
and invoke the <a class="reference external" href="https://rocm.docs.amd.com/projects/HIP/en/latest/.doxygen/docBin/html/group___device.html#ga43c1e7f15925eeb762195ccb5e063eae">hipSetDevice</a>
interface e.g.:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="kt">int</span><span class="w"> </span><span class="n">accelerator_0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">...;</span>
<span class="kt">int</span><span class="w"> </span><span class="n">accelerator_1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">...;</span>

<span class="kt">bool</span><span class="w"> </span><span class="nf">multiple_accelerators</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;&amp;</span><span class="w"> </span><span class="n">u</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;&amp;</span><span class="w"> </span><span class="n">v</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">atomic</span><span class="o">&lt;</span><span class="kt">unsigned</span><span class="w"> </span><span class="kt">int</span><span class="o">&gt;</span><span class="w"> </span><span class="n">r</span><span class="p">{</span><span class="mi">0u</span><span class="p">};</span>

<span class="w">  </span><span class="kr">thread</span><span class="w"> </span><span class="n">t0</span><span class="p">{[</span><span class="o">&amp;</span><span class="p">]()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">hipSetDevice</span><span class="p">(</span><span class="n">accelerator_0</span><span class="p">);</span>

<span class="w">    </span><span class="n">r</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">count</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">execution</span><span class="o">::</span><span class="n">par_unseq</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">cbegin</span><span class="p">(</span><span class="n">u</span><span class="p">),</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">cend</span><span class="p">(</span><span class="n">u</span><span class="p">),</span><span class="w"> </span><span class="mi">42</span><span class="p">);</span>
<span class="w">  </span><span class="p">}};</span>
<span class="w">  </span><span class="kr">thread</span><span class="w"> </span><span class="n">t1</span><span class="p">{[</span><span class="o">&amp;</span><span class="p">]()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">hitSetDevice</span><span class="p">(</span><span class="n">accelerator_1</span><span class="p">);</span>

<span class="w">    </span><span class="n">r</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">count</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">execution</span><span class="o">::</span><span class="n">par_unseq</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">cbegin</span><span class="p">(</span><span class="n">v</span><span class="p">),</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">cend</span><span class="p">(</span><span class="n">v</span><span class="p">),</span><span class="w"> </span><span class="mi">314152</span><span class="p">)</span>
<span class="w">  </span><span class="p">}};</span>

<span class="w">  </span><span class="n">t0</span><span class="p">.</span><span class="n">join</span><span class="p">();</span>
<span class="w">  </span><span class="n">t1</span><span class="p">.</span><span class="n">join</span><span class="p">();</span>

<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">r</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Note that this is a temporary, unsafe workaround for a deficiency in the C++
Standard.</p>
</li>
</ol>
</section>
</section>
<section id="open-questions-future-developments">
<h2><a class="toc-backref" href="#id43" role="doc-backlink">Open Questions / Future Developments</a><a class="headerlink" href="#open-questions-future-developments" title="Link to this heading">¶</a></h2>
<ol class="arabic simple">
<li><p>The restriction on the use of global / namespace scope / <code class="docutils literal notranslate"><span class="pre">static</span></code> /
<code class="docutils literal notranslate"><span class="pre">thread</span></code> storage duration variables in offloaded algorithms will be lifted
in the future, when running in <strong>HMM Mode</strong>;</p></li>
<li><p>The restriction on the use of dynamic memory allocation in offloaded
algorithms will be lifted in the future.</p></li>
<li><p>The restriction on the use of pointers to function, and associated features
such as dynamic polymorphism might be lifted in the future, when running in
<strong>HMM Mode</strong>;</p></li>
<li><p>Offload support might be extended to cases where the <code class="docutils literal notranslate"><span class="pre">parallel_policy</span></code> is
used for some or all targets.</p></li>
</ol>
</section>
<section id="spir-v-support-on-hipamd-toolchain">
<h2><a class="toc-backref" href="#id44" role="doc-backlink">SPIR-V Support on HIPAMD ToolChain</a><a class="headerlink" href="#spir-v-support-on-hipamd-toolchain" title="Link to this heading">¶</a></h2>
<p>The HIPAMD ToolChain supports targeting
<a class="reference external" href="https://llvm.org/docs/SPIRVUsage.html#target-triples">AMDGCN Flavoured SPIR-V</a>.
The support for SPIR-V in the ROCm and HIPAMD ToolChain is under active
development.</p>
<section id="compilation-process">
<h3><a class="toc-backref" href="#id45" role="doc-backlink">Compilation Process</a><a class="headerlink" href="#compilation-process" title="Link to this heading">¶</a></h3>
<p>When compiling HIP programs with the intent of utilizing SPIR-V, the process
diverges from the traditional compilation flow:</p>
<section id="using-offload-arch-amdgcnspirv">
<h4><a class="toc-backref" href="#id46" role="doc-backlink">Using <code class="docutils literal notranslate"><span class="pre">--offload-arch=amdgcnspirv</span></code></a><a class="headerlink" href="#using-offload-arch-amdgcnspirv" title="Link to this heading">¶</a></h4>
<ul class="simple">
<li><p><strong>Target Triple</strong>: The <code class="docutils literal notranslate"><span class="pre">--offload-arch=amdgcnspirv</span></code> flag instructs the
compiler to use the target triple <code class="docutils literal notranslate"><span class="pre">spirv64-amd-amdhsa</span></code>. This approach does
generates generic AMDGCN SPIR-V which retains architecture specific elements
without hardcoding them, thus allowing for optimal target specific code to be
generated at run time, when the concrete target is known.</p></li>
<li><p><strong>LLVM IR Translation</strong>: The program is compiled to LLVM Intermediate
Representation (IR), which is subsequently translated into SPIR-V. In the
future, this translation step will be replaced by direct SPIR-V emission via
the SPIR-V Back-end.</p></li>
<li><p><strong>Clang Offload Bundler</strong>: The resulting SPIR-V is embedded in the Clang
offload bundler with the bundle ID <code class="docutils literal notranslate"><span class="pre">hip-spirv64-amd-amdhsa--amdgcnspirv</span></code>.</p></li>
</ul>
</section>
</section>
<section id="architecture-specific-macros">
<h3><a class="toc-backref" href="#id47" role="doc-backlink">Architecture Specific Macros</a><a class="headerlink" href="#architecture-specific-macros" title="Link to this heading">¶</a></h3>
<p>None of the architecture specific <a class="reference internal" href="AMDGPUSupport.html"><span class="doc">AMDGPU macros</span></a> are
defined when targeting SPIR-V. An alternative, more flexible mechanism to enable
doing per target / per feature code selection will be added in the future.</p>
</section>
</section>
</section>


      </div>
      <div class="bottomnav" role="navigation" aria-label="bottom navigation">
      
        <p>
        «&#160;&#160;<a href="SYCLSupport.html">SYCL Compiler and Runtime architecture design</a>
        &#160;&#160;::&#160;&#160;
        <a class="uplink" href="index.html">Contents</a>
        &#160;&#160;::&#160;&#160;
        <a href="HLSL/HLSLDocs.html">HLSL Design and Implementation</a>&#160;&#160;»
        </p>

      </div>

    <div class="footer" role="contentinfo">
    &#169; Copyright 2007-2025, The Clang Team.
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.6.
    </div>
  </body>
</html>